# ü§ñ DocChat ‚Äî RAG Personal Assistant

A full-stack **Retrieval-Augmented Generation (RAG)** application that lets you have intelligent conversations with your own documents (PDF/DOCX). Upload a file, ask questions in natural language, and get accurate, context-aware answers powered by OpenAI ‚Äî with source citations included.

![DocChat UI](ui.png)

---

## ‚ú® Features

- üìÑ **Document Upload** ‚Äî Supports PDF and DOCX files
- üîç **Semantic Search** ‚Äî Finds the most relevant document chunks using vector similarity
- üß† **RAG Pipeline** ‚Äî Grounds LLM responses in your actual document content
- üí¨ **Conversational Interface** ‚Äî Clean chat UI built with React + Vite
- üìå **Source Citations** ‚Äî Every answer links back to the source chunks used
- üóÑÔ∏è **Vector Store** ‚Äî Persistent embeddings via ChromaDB + LangChain
- üî° **Word Embeddings** ‚Äî GloVe/Gensim integration for semantic understanding

---

## üèóÔ∏è Tech Stack

| Layer | Technology |
|---|---|
| Frontend | React, Vite |
| Backend | Python (FastAPI) |
| LLM | OpenAI API (GPT) |
| Vector Store | ChromaDB + LangChain |
| Embeddings | OpenAI Embeddings + Gensim (GloVe) |
| Document Parsing | LangChain document loaders |
| Database | PostgreSQL |

---

## üöÄ Getting Started

### Prerequisites

- Python 3.12+
- Node.js 18+
- PostgreSQL
- OpenAI API Key

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/rag-personal-assistant.git
cd rag-personal-assistant

# Backend setup
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# Frontend setup
cd web
npm install
```

### Environment Variables

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your_openai_api_key_here
DATABASE_URL=postgresql://user:password@localhost:5432/ragdb
```

### Running the App

```bash
# Start the backend API (from root)
python main.py

# Start the frontend (in a separate terminal)
cd web
npm run dev
```

The app will be available at `http://localhost:3004`.

---

## üìê Architecture

```
User Upload (PDF/DOCX)
        ‚îÇ
        ‚ñº
  Document Loader (LangChain)
        ‚îÇ
        ‚ñº
  Text Chunking (26 chunks)
        ‚îÇ
        ‚ñº
  OpenAI Embeddings
        ‚îÇ
        ‚ñº
  ChromaDB Vector Store ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îÇ
  User Query ‚îÄ‚îÄ‚ñ∫ Embed Query ‚îÄ‚îÄ‚ñ∫ Similarity Search
                                          ‚îÇ
                                          ‚ñº
                              Top-K Relevant Chunks
                                          ‚îÇ
                                          ‚ñº
                              Build Prompt + Context
                                          ‚îÇ
                                          ‚ñº
                                  OpenAI GPT API
                                          ‚îÇ
                                          ‚ñº
                              Answer + Source Citations
```

---

## üìÇ Project Structure

```
rag-personal-assistant/
‚îú‚îÄ‚îÄ api/                    # FastAPI backend
‚îÇ   ‚îî‚îÄ‚îÄ main.py             # API routes (/upload, /query)
‚îú‚îÄ‚îÄ web/                    # React frontend
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îî‚îÄ‚îÄ App.jsx         # Main chat interface
‚îú‚îÄ‚îÄ chroma_langchain_db/    # Persistent vector store
‚îú‚îÄ‚îÄ main.py                 # Application entry point
‚îú‚îÄ‚îÄ rag_agent.py            # RAG orchestration logic
‚îú‚îÄ‚îÄ rag_gensim.py           # Gensim/GloVe word embeddings
‚îú‚îÄ‚îÄ semantic_search.py      # Semantic search utilities
‚îî‚îÄ‚îÄ pyproject.toml          # Dependencies
```

---

## üí° How It Works

1. **Upload** a PDF or DOCX document via the UI
2. The backend **parses and chunks** the document into segments
3. Each chunk is **embedded** using OpenAI's embedding model and stored in ChromaDB
4. When you ask a question, it's **embedded** and compared against stored vectors
5. The **top matching chunks** are retrieved and injected into the LLM prompt
6. OpenAI generates an answer **grounded in your document**, with source references returned to the UI

---

## üîÆ Future Improvements

- [ ] Multi-document support and cross-document queries
- [ ] Streaming responses for real-time output
- [ ] User authentication and personal document vaults
- [ ] Support for more file formats (Excel, PowerPoint, TXT)
- [ ] Hybrid search (semantic + keyword BM25)

---

## üìù License

MIT License ‚Äî feel free to use, modify, and distribute.

---

> Built with ‚ù§Ô∏è using LangChain, ChromaDB, OpenAI, and React.
